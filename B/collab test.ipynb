{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpySVYWJhxaV"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_kxfdP4hJUPB",
        "outputId": "d824651c-5aff-464a-efe5-9f60eeb65b89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/611.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/611.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.0)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-addons\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "_KMCCQF_Wwtz",
        "outputId": "5d527bac-bd89-4013-991d-980d3e6e0d94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tnxXKDjq3jEL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3d65f1b-7793-4a93-f26c-fa1ba1b00138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import io\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JMAHz7kJXc5N"
      },
      "outputs": [],
      "source": [
        "class DataProcessing:\n",
        "    def __init__(self):\n",
        "        self.inp_lang_tokenizer = None\n",
        "        self.targ_lang_tokenizer = None\n",
        "\n",
        "    ## Step 1 and Step 2\n",
        "    def sentence_processing(self, sentence):\n",
        "        sentence = sentence.lower().strip()\n",
        "        sentence = self.unicode_to_ascii(sentence)\n",
        "\n",
        "        sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
        "        sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "        sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
        "        sentence = sentence.strip()\n",
        "\n",
        "        #SOS = <, EOS = >\n",
        "        sentence = '<' + sentence + '>'\n",
        "        return sentence\n",
        "\n",
        "    def unicode_to_ascii(self, s):\n",
        "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "    def tokenize(self, inp_lang, targ_lang):\n",
        "        lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='#',char_level=True)\n",
        "        lang_tokenizer.fit_on_texts(targ_lang)\n",
        "\n",
        "        tensor = lang_tokenizer.texts_to_sequences(inp_lang)\n",
        "        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "        tensor2 = lang_tokenizer.texts_to_sequences(targ_lang)\n",
        "        tensor2 = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "        return tensor, lang_tokenizer, tensor2, lang_tokenizer\n",
        "\n",
        "    def load_dataset(self, num_examples=None):\n",
        "        df = pd.read_csv(\"Misspelling_Corpus.csv\")\n",
        "        df['Original'] = df['Original'].apply(self.sentence_processing)\n",
        "        df['Misspelt'] = df['Misspelt'].apply(self.sentence_processing)\n",
        "\n",
        "        targ_lang = df['Original'].values\n",
        "\n",
        "\n",
        "        inp_lang = column_array = df['Misspelt'].values\n",
        "\n",
        "\n",
        "        input_tensor, inp_lang_tokenizer,target_tensor, targ_lang_tokenizer = self.tokenize(inp_lang,targ_lang)\n",
        "\n",
        "        return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
        "\n",
        "    def call(self, num_examples, BUFFER_SIZE, BATCH_SIZE):\n",
        "\n",
        "        input_tensor, target_tensor, self.inp_lang_tokenizer, self.targ_lang_tokenizer = self.load_dataset(num_examples)\n",
        "\n",
        "        input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train))\n",
        "        train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "        val_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))\n",
        "        val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "        return train_dataset, val_dataset, self.inp_lang_tokenizer, self.targ_lang_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EIW4NVBmJ25k"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 100000\n",
        "BATCH_SIZE = 128\n",
        "# Let's limit the #training examples for faster training\n",
        "num_examples = 100000\n",
        "\n",
        "dataset_creator = DataProcessing()\n",
        "train_dataset, val_dataset, inp_lang, targ_lang = dataset_creator.call(num_examples, BUFFER_SIZE, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word, index in inp_lang.word_index.items():\n",
        "    print(f\"Word: {word}, Index: {index}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZasl38uBsEg",
        "outputId": "17115a00-eeb7-47c4-9f76-b9f232fd980f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: #, Index: 1\n",
            "Word:  , Index: 2\n",
            "Word: a, Index: 3\n",
            "Word: e, Index: 4\n",
            "Word: i, Index: 5\n",
            "Word: s, Index: 6\n",
            "Word: o, Index: 7\n",
            "Word: r, Index: 8\n",
            "Word: t, Index: 9\n",
            "Word: n, Index: 10\n",
            "Word: l, Index: 11\n",
            "Word: d, Index: 12\n",
            "Word: m, Index: 13\n",
            "Word: c, Index: 14\n",
            "Word: h, Index: 15\n",
            "Word: b, Index: 16\n",
            "Word: <, Index: 17\n",
            "Word: >, Index: 18\n",
            "Word: p, Index: 19\n",
            "Word: u, Index: 20\n",
            "Word: ., Index: 21\n",
            "Word: v, Index: 22\n",
            "Word: g, Index: 23\n",
            "Word: f, Index: 24\n",
            "Word: y, Index: 25\n",
            "Word: w, Index: 26\n",
            "Word: k, Index: 27\n",
            "Word: ,, Index: 28\n",
            "Word: x, Index: 29\n",
            "Word: j, Index: 30\n",
            "Word: q, Index: 31\n",
            "Word: z, Index: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "w2lCTy4vKOkB",
        "outputId": "8c8cad2b-5cf8-4c76-8bcf-5ebae9c1e774",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([128, 109]),\n",
              " TensorShape([128, 109]),\n",
              " ['< a b e l i a n   g o o p e s   a r e f   g r o u p s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b d o m e n s   h a s e   p a r d   l o b e s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b a   i s   b a s e d   o n   t u   s c i e n c e   o f   p h y s i c o l o g y   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< m o s y   a a r d v a r k s   h a a v   w e i g h t   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   a m e   p a r t   b r e a s t s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< s o m w   a b n o r m a l   e v e n t s   c o u r s e   m u t a t i o n   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< c k m e   a a r d w o l f s   h a f e   d i s t a c e   c a l l s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< b o s t   a a r d v a r k s   l o o k   l i k e   a n i m a l s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b n o r m a l i t i e s   a r i s e   w h e n   s h a   r e g u l a t o r y   g e n e s   g i v e   t h e   w r o n g   s i g n a l   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b d i c a t i o n   i s f l   r e s i g n a t i o n   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   a l s o   d i g   t o   g e t   f i o t   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b d o m i n o p l a s t y   i s   n e m o s t   a l w a y s   c o s m e t i c   s u r g e r y   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b d u c t i v e   c h a n g e   i g   c h a n g e   d u e   ,   i t   i s   c l a i m e d   ,   t o   a b d u c t i o n   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b a l o n e   d o   g a s t r o p o d s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b d i c a t i o n   i s   t h e   w o r k   f o r   s u r r e n d e r i n g   h o r   p r i n c i p l e s   l e g i s l a t i v e l y   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b n o r m a l i t i e s   r e s u l t   f i n d m   i n h e r e n t   d e f e c t s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   a r e   n o c t u r n a l   ,   w h i c h   m e a n s   t h e y   a r e   a w a k   a t   n i g h t   a n d   a s l e e p   d u r i n g   t h e   d a y   . > # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b n o r m a l   p r o t e i n s   s e r v e   h i s   e u k a r y o t i c   s t r e s s   s i g n a l s   a n d   t r i g g e r   t h e   a c t i v a t i o n   o f   h e a t   s h o c k   g e n e s   . > # # # # #',\n",
              "  '< a a r d w o l v e s   c o n s u m e   t h e i r   p r e y   b y   l i c k i n g   t e r m i t e s   o f f   s u r f a c e s   u s i n g   a   f l a t   ,   s t i c k y   s o n g u e   . > # # # # # # # # # # # # # #',\n",
              "  '< m a t e   a a r d v a r k s   e x c a v a t e   o n e p   b u r r o w s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b o r i g i n a l   p e o p l e s   a r e   a s   f r e e   a d n d   e q u a l   a s   n o n   a b o r i g i n a l   c a n a d i a n s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   d i g e   b u r r o w s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< s o   a a r d w o l f s   e a t   m e a t   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b i l t y   g o   c o g n i t i o n   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   h a v v   p a r t   c y t o p l a s m   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   h a v e   p a r d   c e l l s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   h a v e   p a r d   v e r t e b r a t e   f e t   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   a l s o   e a t   l o c u s t s   a n d   a   t i e m p   o f   g r a s s h o p p e r   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b i l i t e y   i n f l u e n c e s   o u t c o m e s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d w o l v e s   a r r   a   m e m b e r   o f   t h e r e   h y e n a   f a m i l l y   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b d o m e n s   l o s   h u m p s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   s m v e   l o n g s   s l e n d e r   n o s e s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b d o m i n a l   a t t a c k s   c o u s e   s e v e r   p a i n e   ,   n a u s e a   ,   v o m i t i n g   ,   a n d   w a t e r y   d i a r r h e a   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   f e d   o n   t e r m i t e s   a n d   a n t s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   e a r   f o d   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< m o n s t   a a r d v a r k s   h a v e   s l e n d e r   n o s e s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   e s e   w o r m l i k e   t o n g u e s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d w o l v e s   l i b e   i n   p a i r s   ,   w i t e   j u s t   t h e i r   m a t e s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b i o t i c   f a c t o r s   a r a   g e o l o g i c a l   ,   g e o g r a p h i c a l   ,   h y d r o l o g i c a l   a   c l i m a t o l o g i c a l   p a r a m e t e r s   . > # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   e x c a v a t e   h w n   b u r r o w s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   l i v e   t h r o u o u t   a f r i c a   ,   s o u t h   o f   l h e   s a h a r a   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b a   i s   u s e d   a s   a   t r e a t m e n t   f o r   s o o m e   p s y c h o l o g i c a l   d i s o r d e r s   a n d   d e v e l o p m e n t a l   d i s s a b i l i t e y s   . > # # # # # # # # # # # # #',\n",
              "  '< a b o l i t i o n i s t s   m i k e   r e f o r m e r s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b n o r m a l   c e e l s   a r e   p r e s e n t   i m   t h e   c e r v i x   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b e l i a n   g o o p e s   a r e   m o d u l e s   o v e r   t h e   r i n g   o f   i n t e g e r s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   a r e   c a r n i v o r e s   a s   t h e y   e e d   u p o n   t e r m i t e s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d w o l f s   g a v   p a r k   s e c i o n   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b o r i g i n a l   p r o p l e   h a v e   a   l o   e d u c a t i o n a l   s t a t u s   c o m p p a r e d   w i t h   t h e   h w o l e   p o p u l a t i o n   . > # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d w o l v e s   e r e   f o u n d   g n   t h e   o p e n   ,   g r a s s y   p l a i n s   o f f   f a s t   a n f   s o u t h   a f r i c a   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d w o l f s   h a v n e   p a r t   e a r s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d w o l v e s   l i v e   e n t i r e l y   o n   r t w o   s p r c i e d   c i f t   t e r m i t e s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   h a f   p a r t   b r a i n s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   s p e n d   m o s t   o f   t h e   d a y   i n   t h e   b u r r o w   a n d   e m e r g e   s h o r t l y   b e f o r e   o r   a f t e r   s u n m e t   m o   f e e d   . > # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   a h v e   s r t   p l a s m a   m e m b r a n e s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b a l o n e   m s   s h e l l f i s h s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b n o r m a l   g r o w t h   r e s u l t   i n   d e v e l o p m e n t   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b a   t h e r a p y   i s   p r o v i e d   b y   a   t a m   w o r k i n g   w i t h   t h e   c h i l d   ,   a n d   s u p p o r t i n g   h a v e   t e m   m e m e r s   . > # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b o r i g i n a l   p e o p l e s   s e e k   a   r e c o g n i t i o n   g f   t h e i r   r i g h t s   a s   p e o p l e s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b o l i t i o n i s m   c u s t s   t c r o s s   s l l   d i v i s i o n s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b n o r m a l i t y   i s   a s   a   c h a n g e   f r o m   g l u t a m i n e   t o   v a l i n e   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b n o r m a l i t i e s   i n c l u d   a v t i v i t i e s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   h a d   u n u s a l   a p p e a r e n c e s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b d o m i n a l   o b e s i t y   i s   p a r t i c u l a r i l y   d a n g e r o u s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b n o r m a l   p o s t u r e   c a u s e s   m e c h a n i c a l   f o r c e   t o   b e   t r a n s f o r m e d   i n t o   e l e c t r i c a l   e n r e r g y   . > # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   w r e   m o s t l y   s o l t a r y   a n d   n o c t u r n a l   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d w o l v e s   a r e   m e   s m a l l e s t   s p e c i e s   o f   h y e n a   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b a s   a r e   b a b r i c   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   a r e   p a r t   v e r t e b r a t e   f e t   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< g o m e   a b n o r m a l i t i e s   c o n t r i b u t e   s w   d e a d h   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a n   a a r d v a r k   i s   b a   m a m m a l > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b a   i s   a   t e c h n i q u e   ,   w h i c h   h a s   p r o v e n   t o   b e   a n   e f f e c t i v e   t e c h i n g   m e t h o d   f o r   c h i l d r e n   w i t h   a u t i s m   . > # # # # # # # # #',\n",
              "  '< a a r d w o l v e s   w r e   m o n o g a m o u s   a n d   b o t h   p a r e n t s   r a i s e   t h e   c u b s   t o g e t h e r   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b d o m i n a l   p a i n   o c c u r s   o n l y u   p r i o r   t o   a n d   d u r i n g   b o w e l   m o v e m e n t s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b b r e v i a t i o n s   r e f e r   t r o   b o o k d   i n   w i c h e   i l l u s t r a t i o n s   a p p e a r   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b n o r m a l   c e e l s   a r e   i r r e g u l a r   i n   s h a p e   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< s i o m e   a b n o r m a l i t i e s   r e s u l t   i n   a n e m i a   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b a l o n e s   a r e   a l s o   v u l n e r a b l e   o n   v e r a e r s   b a c t e r i a l   a n d   s a r e s i t i c   i n f e c t i o n s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< s o m e   a b o r i g i n a l   p e o p l e   c a n   s t i l l i   r e c a l l   t h e i r   f i r s t   s i g h t i n g s   o f   c a m e l s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b o r i g i n a l   p e o p l e   a r e   f o u r   t i m e s   a s   l i k e l y   t o e   s u f f e r   l o s s   o r   d e a t h   b y   h o u e   f i r e   . > # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a n   a b d o m e n   i s   a p o   b o d y   p a r t > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d w o l v e s   l i v e   a l i n e   ,   i n   p a i r s   ,   o u r   i n   f a m i l y   g r o u p s   w i t h   t w o   t a   f o u r   y o u n g   . > # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< m o s t   a a r d v a r k s   e a t   s o l e d   f o o d   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a n   a b d o m i n o p l a s t y   i s   a n   p e r a t i o n > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< m o s t   a b n o r m a l i t i e s   c h a n g e   i n e r   s i m e   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b n o r m a l i t y   w e   i n   a   c h a n g e   f r o m   g l u t a m i n e   t o   v a l i n e   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b e r a t i o n   m s   m e n t a l   d i s o r d e r   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b o r i g i n a l   r i g h t s   a r e   r i g h t s   f o r   a b o r i g i n a l   p e a p e r   ,   g u a r a n t e e d   b y   t h e   c o n s t i t u t i o n   . > # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   n o e   v e r t e b r a t e s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d w o l f s   a h e v   a r t   n u c l e i   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b o r i g i n a l   p e o p l e   s i g n i f y   i n h e r i t a n c e   r i g h t s   f y   p a i n t i n g   i m a g e s   o f   p a r t i c u l a r   l a n d   t r a c t s   . > # # # # # # # # # # # # # # # #',\n",
              "  '< s o m e   a a r d v a r k s   a e a t   f r u i t   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   s p e n d   m o s t   o f   t h e   d a y   i n   t h e   b u r r o w   a n d   e m e r g e   s h o r t l y   b e f o r e   o r   a d t e r   s u n s e t   t o   f e e d   . > # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   a r e   l o c a t e d   t h r o u g h o u t   c e n t r a l   a n d   s o u t h e r n   a f r i c a   ,   s o u t h   o e   t h e   s a h a r a   d e s e r t   . > # # # # # # # # # # # # # # # #',\n",
              "  '< a b o r i g i n a l   a r t w o   i s   a n   i m p o r t a n t   p a r t   o f   t h e   w o r l d s   o l d e s t   c o n t i n o s   c u l t u r a l   t r a d i t i o n   . > # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b a   a l s o   a d j u s t s   r o o t   s i d   s h o o t   s i z e   i n   o r d e r   t o   i n c r e a s e   w a t e r   a n d   m i n e r a l   c o n s u m p t i o n   . > # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b d o m i n a l   p r e s s u r e   i s   a   l o t   r e s o r t   w i t h   i n f a n t s   b e c a u s e   o f   t h e i r   l i v e r   d e v e l o p m e n t   . > # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d w o l f s   a h e v   d i e t s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b n o r m a l   b r e a t h i n g   s o u n d s   ,   s o m e t i m e s   i n c i c a t i n g   a l u i d   o i   t h e   a i r   s a c s   o f   t h e   l u n g   . > # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b n o r m a l   t i s s u e s   a r p e a r   h a s e   d u c r   s p o t s   o n   a   p e t   s c a n   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b a   p r o m o t e s   d o r m a n c y   o n   w i n t e r   b u d s   a n d   s e e d s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b o m i n a t i o n   i g   d i s g u s t > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d w o l f s   o c c u p i e   h a b i t a t s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b o r i g i n a l   p e o p l e   e o s   a w   d i f f e r e n t   w a y   i n   s e e i n g   r e a l y t y   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b a t e m e n t   u i s   m o d e r a t i o n   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< m o s t   a a r d v a r k s   e a t   s o l e d   f o o d   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b d o m i n a l   b r a t h i n g   i r e   b r e a t h i n g   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   o l n y   h a v e   a n   a m o u n t   o f   s h o r t   c o a r s e   h i r e   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b d o m i n a l   p a i n   i s   w w e   p r e d o m i n a n t   s y m p t o m   o f   u l c e r   f i s e a s e   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b a   t h e r a p y   i s   p r o v d e d   b y   a   t e a m   w o r k i n g   w i t h   h i   c h i l d   ,   a n d   s u p p o r t i n g   o t h e r   t e a m   m e m b e r s   . > # # # # # # # # # # # # # # #',\n",
              "  '< a b d o m i n a l   s i t e s   t e n d   t o   l e e k   l e s s   i n s u l i n   t h a n   o t h e r   s i t e s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   a l s   p a r t   t o n g u e s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d w o l v e s   a r e   m o n o g a m o u s   a n d   b o t h   p a r e n t s   r a s e   t h e   c u b s   t o g e t h e r   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< s o m e p   a b n o r m a l i t i e s   a l t e r   p e r m e a b i l i t y   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< m o s t   a b n o r m a l i t i e s   c o u r s e   b o n e   p a i n   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b n o r m a l   b e h a v o u r   c a n   s t e m   f r o m   a   v a r i e t y   o f   a r e a s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b e l i a n   g o n d s   a r e   m o d u l e s   a v e r   a t h e   t i n g   o f   i n t e g e r s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b o r i g i n a l   d e v e l o p m e n t   i s   c r i t i c a l   t o   t h e   o v e r a l l   d e v e l o p m e n t   o f   t h e   n o r t h e r n   t e r r i o r y   . > # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b n o r m a l i t i e s   r e q u a r e   t r e a t m e n t s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d w o l f s   h a v e   a r t   p l a s m a   m e m b r a n e s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   h a v e   d i e t s   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b i o t i c   t r a n s o r t a t i o n   h i s   a c h i e v e d   t h f o u g h   w i n d   ,   e a i n   o r   o t h e r   a c t s   o f   n a t u r e   . > # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b a   i s   t h e   p r o f e s s i o n a l   o r a n i s a t i o n   f o r   t h e   d i s c i p l i n e   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b o r i g i n a l   l o r e   f o r b i d s   t h e   p u b l i c a t i o n   o f   t h e   g i v e n   n o u n   a n d   p h o t o g r a p h   o f   a   d e c e a s e d   p e r s o n   . > # # # # # # # # # # # #',\n",
              "  '< a a r d v a r k s   h a v e   f o u r   t o e s   o n   t h e   f r o n t   f e t   a n d   f i v e   t o e s   o n   t h e i r   b a c k   f e e t   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a a r d w o l v e s   a n r   d i m i n u t i v e   ,   d e l i c a t e   h y a e n a s   t h a t   e e d   e x c l u s i v e l y   o n b   a n d s   a n d   t e r m i t e s   . > # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b n o r m a l i t i e s   c a h a n g e   o v e r t o   t i m e   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a b a   n s   b a s e d   o n   t h o   s e   o f   p s y c h o l o g y   . > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',\n",
              "  '< a n   a b l e i s m   t h   d i s c r i m i n a t i o n > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #'])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "example_input_batch, example_target_batch = next(iter(train_dataset))\n",
        "example_input_texts = [inp_lang.sequences_to_texts([sequence.numpy()])[0] for sequence in example_input_batch]\n",
        "example_input_batch.shape, example_target_batch.shape, example_input_texts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgCLkfv5uO3d"
      },
      "source": [
        "### Some important parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TqHsArVZ3jFS"
      },
      "outputs": [],
      "source": [
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "max_length_input = example_input_batch.shape[1]\n",
        "max_length_output = example_target_batch.shape[1]\n",
        "\n",
        "embedding_dimentions = 256\n",
        "units = 1024\n",
        "steps_per_epoch = num_examples//BATCH_SIZE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "g-yY9c6aIu1h",
        "outputId": "fefc6f56-2ab9-49a8-f91b-2ada2d8bdf10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input len, output len, vocab size\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(109, 109, 33)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "print(\"input len, output len, vocab size\")\n",
        "max_length_input, max_length_output, vocab_inp_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nZ2rI24i3jFg"
      },
      "outputs": [],
      "source": [
        "#####\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, encoder_cell = \"LSTM\"):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "\n",
        "    if encoder_cell == \"RNN\":\n",
        "        self.encoder_layer = tf.keras.layers.SimpleRNN(self.enc_units,\n",
        "                                                      return_sequences=True,\n",
        "                                                      return_state=True,\n",
        "                                                      recurrent_initializer='glorot_uniform')\n",
        "    elif encoder_cell == \"GRU\":\n",
        "        self.encoder_layer = tf.keras.layers.GRU(self.enc_units,\n",
        "                                                  return_sequences=True,\n",
        "                                                  return_state=True,\n",
        "                                                  recurrent_initializer='glorot_uniform')\n",
        "    else:\n",
        "      self.lstm_layer = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, h, c = self.lstm_layer(x, initial_state = hidden)\n",
        "    return output, h, c\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "60gSVh05Jl6l",
        "outputId": "604621bd-dfbd-4f42-9d4c-3bfc7ef32260",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (128, 109, 1024)\n",
            "Encoder h vecotr shape: (batch size, units) (128, 1024)\n",
            "Encoder c vector shape: (batch size, units) (128, 1024)\n"
          ]
        }
      ],
      "source": [
        "## Test Encoder Stack\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dimentions, units, BATCH_SIZE)\n",
        "\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder h vecotr shape: (batch size, units) {}'.format(sample_h.shape))\n",
        "print ('Encoder c vector shape: (batch size, units) {}'.format(sample_c.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yJ_B3mhW3jFk"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, attention_type='luong'):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.attention_type = attention_type\n",
        "\n",
        "    # Embedding Layer\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)\n",
        "    self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "\n",
        "    # Create attention mechanism with memory = None\n",
        "    mem_seq_len = self.batch_sz*[max_length_input]\n",
        "\n",
        "    if(self.attention_type=='bahdanau'):\n",
        "      # modified paper\n",
        "      self.attention_mechanism = tfa.seq2seq.BahdanauAttention(units=self.dec_units, memory=None, memory_sequence_length=mem_seq_len)\n",
        "    else:\n",
        "      #original paper\n",
        "      self.attention_mechanism = tfa.seq2seq.LuongAttention(units=self.dec_units, memory=None, memory_sequence_length=mem_seq_len)\n",
        "\n",
        "\n",
        "    self.rnn_cell = self.build_rnn_cell(batch_sz)\n",
        "\n",
        "    # Define the decoder with respect to fundamental rnn cell\n",
        "    self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)\n",
        "\n",
        "\n",
        "  def build_rnn_cell(self, batch_sz):\n",
        "    rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell,\n",
        "                                  self.attention_mechanism, attention_layer_size=self.dec_units)\n",
        "    return rnn_cell\n",
        "\n",
        "  def build_initial_state(self, batch_sz, encoder_state, Dtype):\n",
        "    decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)\n",
        "    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
        "    return decoder_initial_state\n",
        "\n",
        "\n",
        "  def call(self, inputs, initial_state):\n",
        "    x = self.embedding(inputs)\n",
        "    outputs, _, _ = self.decoder(x, initial_state=initial_state, sequence_length=self.batch_sz*[max_length_output-1])\n",
        "    return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DaiO0Z6_Ml1c",
        "outputId": "e0c483ad-3142-42b8-d1c6-4903b3ce3ad6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder Outputs Shape:  (128, 108, 33)\n"
          ]
        }
      ],
      "source": [
        "# Test decoder stack\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dimentions, units, BATCH_SIZE, 'luong')\n",
        "sample_x = tf.random.uniform((BATCH_SIZE, max_length_output))\n",
        "decoder.attention_mechanism.setup_memory(sample_output)\n",
        "initial_state = decoder.build_initial_state(BATCH_SIZE, [sample_h, sample_c], tf.float32)\n",
        "\n",
        "\n",
        "sample_decoder_outputs = decoder(sample_x, initial_state)\n",
        "\n",
        "print(\"Decoder Outputs Shape: \", sample_decoder_outputs.rnn_output.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "## Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WmTHr5iV3jFr"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "  loss = cross_entropy(y_true=real, y_pred=pred)\n",
        "\n",
        "  # calculate mask loss accuracy\n",
        "  mask = tf.logical_not(tf.math.equal(real,0))\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "\n",
        "  loss = mask* loss\n",
        "  loss = tf.reduce_mean(loss)\n",
        "  return loss\n",
        "\n",
        "\n",
        "def masked_accuracy(real, pred):\n",
        "    # Ignore padding tokens\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    correct_predictions = tf.cast(tf.equal(real, tf.argmax(pred, axis=-1, output_type=tf.int32)), dtype=tf.float32)\n",
        "    correct_predictions *= mask\n",
        "    accuracy = tf.reduce_sum(correct_predictions) / tf.maximum(tf.reduce_sum(mask), 1)\n",
        "\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMVWzzsfNl4e"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Zj8bXQTgNwrF"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = '/content/gdrive/MyDrive/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Bw95utNiFHa"
      },
      "source": [
        "## One train_step operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "sC9ArXSsVfqn"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "    acc = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
        "\n",
        "        # Ignore SOS and EOS tokens (<,>)\n",
        "        dec_input = targ[:, :-1]\n",
        "        real = targ[:, 1:]\n",
        "\n",
        "        decoder.attention_mechanism.setup_memory(enc_output)\n",
        "        decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\n",
        "        pred = decoder(dec_input, decoder_initial_state)\n",
        "        logits = pred.rnn_output\n",
        "        loss = loss_function(real, logits)\n",
        "        acc = masked_accuracy(real, logits)\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return loss, acc\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def validation_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "    acc = 0\n",
        "\n",
        "    enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_input = targ[:, :-1]\n",
        "    real = targ[:, 1:]\n",
        "\n",
        "    decoder.attention_mechanism.setup_memory(enc_output)\n",
        "    decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\n",
        "    pred = decoder(dec_input, decoder_initial_state)\n",
        "    logits = pred.rnn_output\n",
        "    loss = loss_function(real, logits)\n",
        "    acc = masked_accuracy(real, logits)\n",
        "\n",
        "    return loss, acc\n"
      ],
      "metadata": {
        "id": "ejug-hXWDJe3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pey8eb9piMMg"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ddefjBMa3jF0",
        "outputId": "af47a404-825c-4fe6-bfde-95e5061a1719",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 1.9315 Accuracy 0.0319\n",
            "Epoch 1 Batch 100 Loss 1.1168 Accuracy 0.3878\n",
            "Epoch 1 Batch 200 Loss 0.8255 Accuracy 0.5204\n",
            "Epoch 1 Batch 300 Loss 0.5785 Accuracy 0.6579\n",
            "Epoch 1 Batch 400 Loss 0.3826 Accuracy 0.7997\n",
            "Epoch 1 Batch 500 Loss 0.2535 Accuracy 0.8810\n",
            "Epoch 1 Batch 600 Loss 0.2054 Accuracy 0.9011\n",
            "Epoch 1 Loss 0.5168 Accuracy 0.5140\n",
            "Validation Loss 0.2064, Validation Accuracy 0.9036\n",
            "Time taken for 1 epoch 573.3168127536774 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.2136 Accuracy 0.9069\n",
            "Epoch 2 Batch 100 Loss 0.1927 Accuracy 0.9165\n",
            "Epoch 2 Batch 200 Loss 0.1572 Accuracy 0.9298\n",
            "Epoch 2 Batch 300 Loss 0.1607 Accuracy 0.9229\n",
            "Epoch 2 Batch 400 Loss 0.1522 Accuracy 0.9308\n",
            "Epoch 2 Batch 500 Loss 0.1328 Accuracy 0.9327\n",
            "Epoch 2 Batch 600 Loss 0.1450 Accuracy 0.9317\n",
            "Epoch 2 Loss 0.1276 Accuracy 0.7408\n",
            "Validation Loss 0.1367, Validation Accuracy 0.9370\n",
            "Time taken for 1 epoch 564.8701152801514 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.1418 Accuracy 0.9305\n",
            "Epoch 3 Batch 100 Loss 0.1315 Accuracy 0.9403\n",
            "Epoch 3 Batch 200 Loss 0.1383 Accuracy 0.9391\n",
            "Epoch 3 Batch 300 Loss 0.1256 Accuracy 0.9416\n",
            "Epoch 3 Batch 400 Loss 0.1002 Accuracy 0.9531\n",
            "Epoch 3 Batch 500 Loss 0.1346 Accuracy 0.9404\n",
            "Epoch 3 Batch 600 Loss 0.1202 Accuracy 0.9464\n",
            "Epoch 3 Loss 0.0951 Accuracy 0.7560\n",
            "Validation Loss 0.1165, Validation Accuracy 0.9469\n",
            "Time taken for 1 epoch 564.0054440498352 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.0946 Accuracy 0.9522\n",
            "Epoch 4 Batch 100 Loss 0.1111 Accuracy 0.9475\n",
            "Epoch 4 Batch 200 Loss 0.1109 Accuracy 0.9484\n",
            "Epoch 4 Batch 300 Loss 0.1117 Accuracy 0.9474\n",
            "Epoch 4 Batch 400 Loss 0.1044 Accuracy 0.9518\n",
            "Epoch 4 Batch 500 Loss 0.1149 Accuracy 0.9472\n",
            "Epoch 4 Batch 600 Loss 0.1075 Accuracy 0.9513\n",
            "Epoch 4 Loss 0.0821 Accuracy 0.7619\n",
            "Validation Loss 0.1072, Validation Accuracy 0.9516\n",
            "Time taken for 1 epoch 564.3900787830353 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.0911 Accuracy 0.9566\n",
            "Epoch 5 Batch 100 Loss 0.0958 Accuracy 0.9538\n",
            "Epoch 5 Batch 200 Loss 0.0970 Accuracy 0.9559\n",
            "Epoch 5 Batch 300 Loss 0.1023 Accuracy 0.9553\n",
            "Epoch 5 Batch 400 Loss 0.0925 Accuracy 0.9541\n",
            "Epoch 5 Batch 500 Loss 0.1082 Accuracy 0.9554\n",
            "Epoch 5 Batch 600 Loss 0.0980 Accuracy 0.9538\n",
            "Epoch 5 Loss 0.0753 Accuracy 0.7651\n",
            "Validation Loss 0.1018, Validation Accuracy 0.9538\n",
            "Time taken for 1 epoch 563.8887755870819 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.0990 Accuracy 0.9535\n",
            "Epoch 6 Batch 100 Loss 0.0902 Accuracy 0.9573\n",
            "Epoch 6 Batch 200 Loss 0.0977 Accuracy 0.9574\n",
            "Epoch 6 Batch 300 Loss 0.0980 Accuracy 0.9545\n",
            "Epoch 6 Batch 400 Loss 0.0892 Accuracy 0.9589\n",
            "Epoch 6 Batch 500 Loss 0.0935 Accuracy 0.9596\n",
            "Epoch 6 Batch 600 Loss 0.0892 Accuracy 0.9602\n",
            "Epoch 6 Loss 0.0704 Accuracy 0.7674\n",
            "Validation Loss 0.0972, Validation Accuracy 0.9562\n",
            "Time taken for 1 epoch 564.7369179725647 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.0867 Accuracy 0.9608\n",
            "Epoch 7 Batch 100 Loss 0.0857 Accuracy 0.9621\n",
            "Epoch 7 Batch 200 Loss 0.0841 Accuracy 0.9616\n",
            "Epoch 7 Batch 300 Loss 0.0792 Accuracy 0.9613\n",
            "Epoch 7 Batch 400 Loss 0.0818 Accuracy 0.9634\n",
            "Epoch 7 Batch 500 Loss 0.0980 Accuracy 0.9571\n",
            "Epoch 7 Batch 600 Loss 0.0959 Accuracy 0.9565\n",
            "Epoch 7 Loss 0.0664 Accuracy 0.7697\n",
            "Validation Loss 0.0929, Validation Accuracy 0.9590\n",
            "Time taken for 1 epoch 563.9497356414795 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0750 Accuracy 0.9636\n",
            "Epoch 8 Batch 100 Loss 0.0698 Accuracy 0.9657\n",
            "Epoch 8 Batch 200 Loss 0.0810 Accuracy 0.9605\n",
            "Epoch 8 Batch 300 Loss 0.0822 Accuracy 0.9618\n",
            "Epoch 8 Batch 400 Loss 0.0742 Accuracy 0.9644\n",
            "Epoch 8 Batch 500 Loss 0.1032 Accuracy 0.9583\n",
            "Epoch 8 Batch 600 Loss 0.0676 Accuracy 0.9647\n",
            "Epoch 8 Loss 0.0622 Accuracy 0.7715\n",
            "Validation Loss 0.0755, Validation Accuracy 0.9645\n",
            "Time taken for 1 epoch 568.920325756073 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0627 Accuracy 0.9695\n",
            "Epoch 9 Batch 100 Loss 0.0230 Accuracy 0.9867\n",
            "Epoch 9 Batch 200 Loss 0.3211 Accuracy 0.9018\n",
            "Epoch 9 Batch 300 Loss 28.0118 Accuracy 0.0819\n",
            "Epoch 9 Batch 400 Loss 2.4612 Accuracy 0.1421\n",
            "Epoch 9 Batch 500 Loss 1.5999 Accuracy 0.1583\n",
            "Epoch 9 Batch 600 Loss 1.4812 Accuracy 0.1766\n",
            "Epoch 9 Loss 4.3252 Accuracy 0.3510\n",
            "Validation Loss 1.4755, Validation Accuracy 0.1890\n",
            "Time taken for 1 epoch 559.9086649417877 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.5095 Accuracy 0.1872\n",
            "Epoch 10 Batch 100 Loss 1.5468 Accuracy 0.1992\n",
            "Epoch 10 Batch 200 Loss 1.3877 Accuracy 0.2112\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-11616f9e5d93>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Store batch losses and accuracies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \"\"\"\n\u001b[1;32m    393\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    358\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "# Lists to store additional variables\n",
        "losses = []\n",
        "accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
        "        batch_loss, batch_acc = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "        total_accuracy += batch_acc\n",
        "\n",
        "        # Store batch losses and accuracies\n",
        "        losses.append(batch_loss.numpy())\n",
        "        accuracies.append(batch_acc.numpy())\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\n",
        "                                                                         batch,\n",
        "                                                                         batch_loss.numpy(),\n",
        "                                                                         batch_acc.numpy()))\n",
        "\n",
        "    # Validation pass\n",
        "    val_total_loss = 0\n",
        "    val_total_accuracy = 0\n",
        "    num_val_batches = 0\n",
        "\n",
        "    for val_inp, val_targ in val_dataset:\n",
        "        val_batch_loss, val_batch_acc = validation_step(val_inp, val_targ, enc_hidden)\n",
        "        val_total_loss += val_batch_loss\n",
        "        val_total_accuracy += val_batch_acc\n",
        "        num_val_batches += 1\n",
        "\n",
        "    val_loss = val_total_loss / num_val_batches\n",
        "    val_accuracy = val_total_accuracy / num_val_batches\n",
        "\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    # Saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n",
        "    print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\n",
        "                                                         total_loss / steps_per_epoch,\n",
        "                                                         total_accuracy / steps_per_epoch))\n",
        "    print('Validation Loss {:.4f}, Validation Accuracy {:.4f}'.format(val_loss, val_accuracy))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "# After training, you can process and analyze the recorded metrics as needed\n",
        "# For example, print the losses, accuracies, validation losses, and validation accuracies\n",
        "print(\"Train Losses:\", losses)\n",
        "print(\"Train Accuracies:\", accuracies)\n",
        "print(\"Validation Losses:\", val_losses)\n",
        "print(\"Validation Accuracies:\", val_accuracies)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU3Ce8M6I3rz"
      },
      "source": [
        "## Use tf-addons BasicDecoder for decoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EbQpyYs13jF_"
      },
      "outputs": [],
      "source": [
        "def evaluate_sentence(sentence):\n",
        "    sentence = dataset_creator.sentence_processing(sentence)\n",
        "\n",
        "    # Tokenize input sentence at the character level\n",
        "    inputs = [inp_lang.word_index[char] for char in sentence]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                          maxlen=max_length_input,\n",
        "                                                          padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    inference_batch_size = inputs.shape[0]\n",
        "    result = ''\n",
        "\n",
        "    enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size, units))]\n",
        "    enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
        "\n",
        "    dec_h = enc_h\n",
        "    dec_c = enc_c\n",
        "\n",
        "    start_tokens = tf.fill([inference_batch_size], targ_lang.word_index['<'])\n",
        "    end_token = targ_lang.word_index['>']\n",
        "\n",
        "    greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
        "\n",
        "    # Instantiate BasicDecoder object\n",
        "    decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.rnn_cell, sampler=greedy_sampler, output_layer=decoder.fc)\n",
        "\n",
        "    # Setup Memory and inital state in decoder\n",
        "    decoder.attention_mechanism.setup_memory(enc_out)\n",
        "    decoder_initial_state = decoder.build_initial_state(inference_batch_size, [enc_h, enc_c], tf.float32)\n",
        "    decoder_embedding_matrix = decoder.embedding.variables[0]\n",
        "\n",
        "    # decode\n",
        "    outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens=start_tokens, end_token=end_token, initial_state=decoder_initial_state)\n",
        "    return outputs.sample_id.numpy()\n",
        "\n",
        "def spellcheck(sentence):\n",
        "    result = evaluate_sentence(sentence)\n",
        "    result_text = targ_lang.sequences_to_texts(result)\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result_text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n250XbnjOaqP"
      },
      "source": [
        "## Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "UJpT9D5_OgP6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30e1bd0e-01cd-4474-f055-ee5ef2e1ec2e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f8b598dca00>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "WYmYhNN_faR5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ec80d56-17c7-4d4a-8e3b-56cd0d34fd0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: aa batteries maintain the settings if the power ever gos off.\n",
            "Predicted translation: ['a a   b a t t e r i e s   m a i n t a i n   t h e   s e t t i n g s   i f   t h e   p o w e r   e v e r   g o e s   o f f   . >']\n"
          ]
        }
      ],
      "source": [
        "spellcheck(u'aa batteries maintain the settings if the power ever gos off.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zSx2iM36EZQZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0492874b-0731-48fa-df11-d7940db0328e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: aardvark females appear tobe come ointo season once per yaer.\n",
            "Predicted translation: ['a a r d v a r k   f e m a l e s   a p p e a r   t o   b o i n g   o f   c o n t a n   o n   a f r i c a   ,   s o u r t h   o f   t h e   a t r i a l   a n e   p o s t r o b o l   g a n d   . >']\n"
          ]
        }
      ],
      "source": [
        "spellcheck(u'aardvark females appear tobe come ointo season once per yaer.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "A3LLCx3ZE0Ls",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87bc69b4-1ab0-41c0-8dfa-6a34f96e0a7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: mosy aardvarks have lnog snouts.\n",
            "Predicted translation: ['m o s y   a a r d v a r k s   h a v e   l o n g   s n o u t s   . >']\n"
          ]
        }
      ],
      "source": [
        "spellcheck(u'mosy aardvarks have lnog snouts.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wAd2LH3MPZsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "axfF67EJPZpf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "DUQVLVqUE1YW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d22b86f1-a50b-456f-f82c-832fa302930c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: abdominal paine is relieved by defecation.\n",
            "Predicted translation: ['a b d o m i n a l   p a i n   i s   r e l e i v e d   b y   d e f e c a t i o n   . >']\n"
          ]
        }
      ],
      "source": [
        "spellcheck(u'abdominal paine is relieved by defecation.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRUuNDeY0HiC"
      },
      "source": [
        "## Use tf-addons BeamSearchDecoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "AJ-RTQ0hsJNL"
      },
      "outputs": [],
      "source": [
        "def beam_evaluate_sentence(sentence, beam_width=3):\n",
        "  sentence = dataset_creator.sentence_processing(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[char] for char in sentence]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                          maxlen=max_length_input,\n",
        "                                                          padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "  inference_batch_size = inputs.shape[0]\n",
        "  result = ''\n",
        "\n",
        "  enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n",
        "  enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
        "\n",
        "  dec_h = enc_h\n",
        "  dec_c = enc_c\n",
        "\n",
        "  start_tokens = tf.fill([inference_batch_size], targ_lang.word_index['<'])\n",
        "  end_token = targ_lang.word_index['>']\n",
        "\n",
        "  enc_out = tfa.seq2seq.tile_batch(enc_out, multiplier=beam_width)\n",
        "  decoder.attention_mechanism.setup_memory(enc_out)\n",
        "  print(\"beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] :\", enc_out.shape)\n",
        "\n",
        "  # set decoder_inital_state which is an AttentionWrapperState considering beam_width\n",
        "  hidden_state = tfa.seq2seq.tile_batch([enc_h, enc_c], multiplier=beam_width)\n",
        "  decoder_initial_state = decoder.rnn_cell.get_initial_state(batch_size=beam_width*inference_batch_size, dtype=tf.float32)\n",
        "  decoder_initial_state = decoder_initial_state.clone(cell_state=hidden_state)\n",
        "\n",
        "  # Instantiate BeamSearchDecoder\n",
        "  decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoder.rnn_cell,beam_width=beam_width, output_layer=decoder.fc)\n",
        "  decoder_embedding_matrix = decoder.embedding.variables[0]\n",
        "\n",
        "  # The BeamSearchDecoder object's call() function takes care of everything.\n",
        "  outputs, final_state, sequence_lengths = decoder_instance(decoder_embedding_matrix, start_tokens=start_tokens, end_token=end_token, initial_state=decoder_initial_state)\n",
        "\n",
        "\n",
        "  # Convert the shape of outputs and beam_scores to (inference_batch_size, beam_width, time_step_outputs)\n",
        "  final_outputs = tf.transpose(outputs.predicted_ids, perm=(0,2,1))\n",
        "  beam_scores = tf.transpose(outputs.beam_search_decoder_output.scores, perm=(0,2,1))\n",
        "\n",
        "  return final_outputs.numpy(), beam_scores.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "g_LvXGvX8X-O"
      },
      "outputs": [],
      "source": [
        "def beam_spellcheck(sentence):\n",
        "  result, beam_scores = beam_evaluate_sentence(sentence)\n",
        "  print(result.shape, beam_scores.shape)\n",
        "  for beam, score in zip(result, beam_scores):\n",
        "    print(beam.shape, score.shape)\n",
        "    output = targ_lang.sequences_to_texts(beam)\n",
        "    output = [a[:a.index('>')] for a in output]\n",
        "    beam_score = [a.sum() for a in score]\n",
        "    print('Input: %s' % (sentence))\n",
        "    for i in range(len(output)):\n",
        "      print('{} Predicted Sentence: {}  {}'.format(i+1, output[i], beam_score[i]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "TODnXBleDzzO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "494e5898-6e4e-4323-a2d5-8d68317c74d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] : (3, 109, 1024)\n",
            "(1, 3, 34) (1, 3, 34)\n",
            "(3, 34) (3, 34)\n",
            "Input: mosy aardvarks have lnog snouts.\n",
            "1 Predicted Sentence: m o s y   a a r d v a r k s   h a v e   l o n g   s n o u t s   .   -11.628411293029785\n",
            "2 Predicted Sentence: m o s y   a a r d v a r k s   h a v e   l n o g   s n o u t s   .   -173.80020141601562\n",
            "3 Predicted Sentence: m o s y   a a r d v a r k s   h a v e   l o n h   s n o u t s   .   -201.2129364013672\n"
          ]
        }
      ],
      "source": [
        "beam_spellcheck(u'mosy aardvarks have lnog snouts.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_BezQwENFY3L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72ff7b5e-f1f8-4cb5-80a5-4f2c260567a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] : (3, 109, 1024)\n",
            "(1, 3, 44) (1, 3, 44)\n",
            "(3, 44) (3, 44)\n",
            "Input: abdominal paine is relieved by defecation.\n",
            "1 Predicted Sentence: a b d o m i n a l   p a i n   i s   r e l e i v e d   b y   d e f e c a t i o n   .   -36.465274810791016\n",
            "2 Predicted Sentence: a b d o m i n a l   p a i n n   i s   r e l e i v e d   b y   d e f e c a t i o n   .   -136.0968017578125\n",
            "3 Predicted Sentence: a b d o m i n a l   p a i n e   i s   r e l e i v e d   b y   d e f e c a t i o n   .   -162.90682983398438\n"
          ]
        }
      ],
      "source": [
        "beam_spellcheck(u'abdominal paine is relieved by defecation.')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}